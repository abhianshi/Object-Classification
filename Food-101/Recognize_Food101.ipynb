{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np \n",
    "from glob import glob\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import pyplot as plt\n",
    "import glob\n",
    "from ComputeSIFT import DetectandCompute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading food image file names for testing and training purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFileName():\n",
    "    # Image path for food-101 dataset\n",
    "    food_items = glob.glob('/Users/abhianshusingla/Downloads/food-101/images/*')\n",
    "    \n",
    "    # Curently doing the testing for only 20 food classes\n",
    "    food_items = food_items[:20]\n",
    "    \n",
    "    # Split the image file name food into food_train and food_test \n",
    "    food_train = []\n",
    "    food_test = []\n",
    "\n",
    "    for f in food_items:\n",
    "        food = glob.glob(f + '/*.jpg')\n",
    "        \n",
    "        # Taking first 200 images for training purpose\n",
    "        food_train.append(food[:200])\n",
    "        \n",
    "        # Taking 20 images for each class for testing purpose\n",
    "        food_test.append(food[200:220])\n",
    "        \n",
    "    return food_train, food_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the SIFT features for both training and testing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractSIFT(food):\n",
    "    \n",
    "    # Count of the number of keypoints for a particular image\n",
    "    count_keypoints = []\n",
    "    \n",
    "    # List of image descriptors\n",
    "    # Size equals to the number of images\n",
    "    food_descriptors = []\n",
    "    \n",
    "    # It is a vertical stack of all the images descriptors, so if the total number of descriptors \n",
    "    # for the combined images is 10000, then vertical_stack length is 10000 and each element will have 128 elements\n",
    "    vertical_stack = []\n",
    "    \n",
    "    # Label names of the food data\n",
    "    labels = []\n",
    "    \n",
    "    # food contains the list of the food class name\n",
    "    for i in range(len(food)):\n",
    "        \n",
    "        # food[i] contains the list of images for that particular food class food[i]\n",
    "        for j in range(len(food[i])):\n",
    "            \n",
    "            # keypoints and descriptors calculated using SIFT\n",
    "            _, des = DetectandCompute(food[i][j])\n",
    "            \n",
    "            # Add the descriptors to the list of food_descriptors\n",
    "            food_descriptors.append(des)\n",
    "            \n",
    "            # make a vertical stack of the elements having length 128( feature vector length)\n",
    "            vertical_stack.extend(des)\n",
    "            \n",
    "            # Store the count of number of keypoints detected for a particular image\n",
    "            # Needed while calculating the histogram\n",
    "            count_keypoints.append(des.shape[0])\n",
    "            \n",
    "            # Assigning the label names according to the folder name\n",
    "            label_name = food[i][j].split('/')\n",
    "            label_name = label_name[-2]\n",
    "            labels.append(label_name)\n",
    "            \n",
    "    return count_keypoints, food_descriptors, vertical_stack, labels   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the data so that large values fit within a small range, it will decrease the time in fitting the data for a classifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "def normalize(vertical_stack):\n",
    "    X = np.array(vertical_stack).astype(np.float64)\n",
    "    \n",
    "    # StandardScaler function\n",
    "    X_scaler = StandardScaler().fit(X)\n",
    "    \n",
    "    # Transforming the data within small range\n",
    "    vertical_stack = X_scaler.transform(X)\n",
    "    \n",
    "    return vertical_stack.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering(vertical_stack, clusters_count):\n",
    "    \n",
    "    # K Means Cluster classifier \n",
    "    cluster_clf = KMeans(n_clusters = clusters_count)\n",
    "    \n",
    "    # Predicting the cluster for every keypoint \n",
    "    pred_clusters = cluster_clf.fit_predict(vertical_stack)\n",
    "    \n",
    "    return pred_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary - Histogram of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram_clusters(count_keypoints,pred_clusters,clusters_count):\n",
    "    \n",
    "    # List of Histogram of all the images\n",
    "    # Histogram obtained is train_X or test_X\n",
    "    histogram = []\n",
    "    count = 0\n",
    "    \n",
    "    # count_keypoints is the list of the number of keypoints for an image\n",
    "    for i in range(len(count_keypoints)):\n",
    "        \n",
    "        # Make an array equal to the size of the histogram\n",
    "        hist = np.zeros(clusters_count)\n",
    "        \n",
    "        # Create histogram of clusters \n",
    "        for j in range(count_keypoints[i]):\n",
    "            hist[pred_clusters[count + j]] += 1\n",
    "        \n",
    "        count += count_keypoints[i]\n",
    "        \n",
    "        # Append the calculated hist of the image to histogram list\n",
    "        histogram.append(hist)\n",
    "    return histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function used for extracting the features and assigning those to train_X, train_Y, test_X and test_Y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractData():\n",
    "    food_train, food_test = readFileName()\n",
    "    \n",
    "    print(\"SIFT..\")\n",
    "    print(\"SIFT of train data..\")\n",
    "    count_keypoints_train, food_descriptors_train, vertical_stack_train, train_Y = extractSIFT(food_train)\n",
    "    print(\"SIFT of test data..\")\n",
    "    count_keypoints_test, food_descriptors_test, vertical_stack_test, test_Y = extractSIFT(food_test)\n",
    "    \n",
    "    print(\"Normalization..\")\n",
    "    print(\"Normalization of train data..\")  \n",
    "    vertical_stack_train = normalize(vertical_stack_train)\n",
    "    print(\"Normalization of test data..\")  \n",
    "    vertical_stack_test = normalize(vertical_stack_test)\n",
    "    \n",
    "    print(\"Clustering..\")\n",
    "    clusters_count = 100\n",
    "    print(\"Clustering of train data..\")\n",
    "    pred_clusters_train = clustering(vertical_stack_train, clusters_count)\n",
    "    print(\"Clustering of test data..\")\n",
    "    pred_clusters_test = clustering(vertical_stack_test, clusters_count)\n",
    "    \n",
    "    print(\"Histogram..\")\n",
    "    print(\"Histogram of train data..\")\n",
    "    train_X = histogram_clusters(count_keypoints_train,pred_clusters_train,clusters_count)\n",
    "    print(\"Histogram of test data..\")\n",
    "    test_X = histogram_clusters(count_keypoints_test,pred_clusters_test,clusters_count)\n",
    "    \n",
    "    return train_X, train_Y, test_X, test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIFT..\n",
      "SIFT of train data..\n",
      "SIFT of test data..\n",
      "Normalization..\n",
      "Normalization of train data..\n",
      "Normalization of test data..\n",
      "Clustering..\n",
      "Clustering of train data..\n",
      "Clustering of test data..\n",
      "Histogram..\n",
      "Histogram of train data..\n",
      "Histogram of test data..\n",
      "Data is Extracted for training and testing\n"
     ]
    }
   ],
   "source": [
    "train_X, train_Y, test_X, test_Y = extractData()\n",
    "print(\"Data is Extracted for training and testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine - Classifier\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def SVC_Classifier(train_X, train_Y, test_X, test_Y):\n",
    "    # Classifier parameters\n",
    "    # kernel = \"rbf\" or \"linear\"\n",
    "    # If gamma is ‘auto’ then 1/n_features will be used.\n",
    "    clf = SVC(C = 1.0, kernel = \"rbf\", gamma = \"auto\")\n",
    "\n",
    "    # Fit the classifier with features_train and labels_train\n",
    "    clf.fit(train_X,train_Y)\n",
    "\n",
    "    # Predicting the label values\n",
    "    y_pred = clf.predict(test_X)\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = clf.score(test_X,test_Y)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the food data with SVM is  0.5\n"
     ]
    }
   ],
   "source": [
    "accuracy = SVC_Classifier(train_X, train_Y, test_X, test_Y)\n",
    "print(\"Accuracy of the food data with SVM is \",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def Forest_Classifier(train_X, train_Y, test_X, test_Y):\n",
    "\n",
    "    # If max_depth value is high, then it won't segregate the data clearly into leaf nodes,\n",
    "    # but if the max_depth value is very low, then it it will take long time to train the data\n",
    "    clf = RandomForestClassifier(max_depth = 8)\n",
    "\n",
    "    # Fit the classifier with features_train and labels_train\n",
    "    clf.fit(train_X,train_Y)\n",
    "\n",
    "    # Predicting the label values\n",
    "    pred_Y = clf.predict(test_X)\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = clf.score(test_X,test_Y)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the food data with Forest Classifier is  0.725\n"
     ]
    }
   ],
   "source": [
    "accuracy = Forest_Classifier(train_X, train_Y, test_X, test_Y)\n",
    "print(\"Accuracy of the food data with Forest Classifier is \",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
